{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21497456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/transformers-4.55.3-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/transformers-4.55.3-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/transformers-4.55.3-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/peft-0.17.1-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/peft-0.17.1-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/peft-0.17.1-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/datasets-4.0.0-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/datasets-4.0.0-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/datasets-4.0.0-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/huggingface_hub-0.34.4-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/huggingface_hub-0.34.4-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/huggingface_hub-0.34.4-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Requirement '/kaggle/input/map-utilities/accelerate-1.10.0-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/input/map-utilities/accelerate-1.10.0-py3-none-any.whl\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/kaggle/input/map-utilities/accelerate-1.10.0-py3-none-any.whl'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps /kaggle/input/map-utilities/transformers-4.55.3-py3-none-any.whl\n",
    "!pip install --no-deps /kaggle/input/map-utilities/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl\n",
    "!pip install --no-deps /kaggle/input/map-utilities/peft-0.17.1-py3-none-any.whl\n",
    "!pip install --no-deps /kaggle/input/map-utilities/datasets-4.0.0-py3-none-any.whl\n",
    "!pip install --no-deps /kaggle/input/map-utilities/huggingface_hub-0.34.4-py3-none-any.whl\n",
    "!pip install --no-deps /kaggle/input/map-utilities/accelerate-1.10.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607d7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "if \"/kaggle\" in ROOT_PATH:\n",
    "    ROOT_PATH = \"/kaggle/input\"\n",
    "    sys.path.append(os.path.join(ROOT_PATH, \"map-utilities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832e43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from utils import (\n",
    "    stringify_input,\n",
    "    get_model_name,\n",
    "    get_sequence_classifier,\n",
    "    get_tokenizer,\n",
    "    get_training_arguments,\n",
    "    get_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cae0a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/map-kaggle-competition/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "le = joblib.load(os.path.join(ROOT_PATH, \"label_encoder.joblib\"))\n",
    "n_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 2\n",
    "TOP_K = 3\n",
    "\n",
    "BASE_MODEL = \"oof_models/jhu-clsp-ettin-encoder-1b\"\n",
    "\n",
    "if \"/kaggle\" not in ROOT_PATH:\n",
    "    BASE_MODEL = os.path.join(ROOT_PATH, BASE_MODEL)\n",
    "\n",
    "MODEL_NAME = get_model_name(\"/kaggle\" in ROOT_PATH, ROOT_PATH, BASE_MODEL)\n",
    "ADAPTER_PATH = get_model_name(\"/kaggle\" in ROOT_PATH, ROOT_PATH, \"deepseek-math-7b-instruct-qlora-4bit/transformers/default/2\")\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = False\n",
    "BITS = 4\n",
    "USE_4BIT = BITS == 4\n",
    "USE_8BIT = BITS == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2191ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"train.csv\")\n",
    "TEST_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b036dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a09b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (36696, 7)\n",
      "Testing Shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape:\", train_df.shape)\n",
    "print(\"Testing Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c0b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1468978/1307861462.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df.is_mc_answer_correct = train_df.is_mc_answer_correct.fillna(False)\n",
      "/tmp/ipykernel_1468978/1307861462.py:13: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df.is_mc_answer_correct = test_df.is_mc_answer_correct.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "idx = train_df.Category.str.contains(\"True\", case=False)\n",
    "tmp = train_df.loc[idx].copy()\n",
    "tmp[\"c\"] = tmp.groupby([\"QuestionId\", \"MC_Answer\"]).MC_Answer.transform(\"count\")\n",
    "tmp = tmp.sort_values(\"c\", ascending=False)\n",
    "tmp = tmp.drop_duplicates([\"QuestionId\"])\n",
    "tmp = tmp[[\"QuestionId\", \"MC_Answer\"]]\n",
    "tmp[\"is_mc_answer_correct\"] = True\n",
    "\n",
    "train_df = train_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "train_df.is_mc_answer_correct = train_df.is_mc_answer_correct.fillna(False)\n",
    "\n",
    "test_df = test_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "test_df.is_mc_answer_correct = test_df.is_mc_answer_correct.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a348b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    for obj in list(globals().keys()):\n",
    "        if isinstance(globals()[obj], torch.nn.Module) or isinstance(globals()[obj], torch.Tensor):\n",
    "            del globals()[obj]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc3cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_config():\n",
    "    \"\"\"Setup model configuration for each fold\"\"\"\n",
    "    # LoRA configuration\n",
    "    lora_config = None\n",
    "    if USE_LORA:\n",
    "        R = 8\n",
    "        lora_config = LoraConfig(\n",
    "            r=R,\n",
    "            lora_alpha=R * 4,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"down_proj\",\n",
    "                \"up_proj\",\n",
    "                \"gate_proj\",\n",
    "            ],\n",
    "            lora_dropout=0.05,\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "    # Quantization configuration\n",
    "    q_lora_config = {\"torch_dtype\": torch.bfloat16}\n",
    "    if \"ettin\" not in MODEL_NAME.lower():\n",
    "        q_lora_config[\"device_map\"] = \"auto\"\n",
    "\n",
    "    if USE_QLORA:\n",
    "        kwargs = {}\n",
    "        if USE_4BIT:\n",
    "            kwargs = {\n",
    "                \"load_in_4bit\": True,\n",
    "                \"bnb_4bit_quant_type\": \"nf4\",\n",
    "                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                \"bnb_4bit_use_double_quant\": True,\n",
    "                \"bnb_4bit_quant_storage\": torch.bfloat16,\n",
    "            }\n",
    "        if USE_8BIT:\n",
    "            kwargs = {\"load_in_8bit\": True}\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(**kwargs)\n",
    "        q_lora_config[\"quantization_config\"] = bnb_config\n",
    "\n",
    "    return lora_config, q_lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549db378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_predictions():\n",
    "    \"\"\"Generate test predictions using all fold models\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"GENERATING TEST PREDICTIONS\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Prepare test data\n",
    "    test_df[\"stringified_input\"] = test_df.apply(\n",
    "        lambda row: stringify_input(row, MODEL_NAME), axis=1\n",
    "    )\n",
    "\n",
    "    all_test_predictions = []\n",
    "\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        print(f\"Loading fold {fold_idx + 1} model...\")\n",
    "\n",
    "        # Load tokenizer\n",
    "\n",
    "        model_path = os.path.join(MODEL_NAME, f\"fold_{fold_idx}\")\n",
    "        if USE_LORA:\n",
    "            model_path = MODEL_NAME\n",
    "\n",
    "        tokenizer = get_tokenizer(model_path)\n",
    "\n",
    "        # Prepare test dataset\n",
    "        test_ds = Dataset.from_pandas(test_df[[\"stringified_input\"]])\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"stringified_input\"])\n",
    "\n",
    "        test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "        # Load model and generate predictions\n",
    "        lora_config, q_lora_config = setup_model_config()\n",
    "        seq_model = get_sequence_classifier(model_path, n_classes, q_lora_config)\n",
    "\n",
    "        if USE_LORA:\n",
    "            fold_model_path = os.path.join(ADAPTER_PATH, f\"fold_{fold_idx}\")\n",
    "            seq_model = PeftModel.from_pretrained(seq_model, fold_model_path)\n",
    "\n",
    "        # Create trainer for inference\n",
    "        training_args = get_training_arguments(\n",
    "            bf16_support=\"/kaggle\" not in ROOT_PATH,\n",
    "        )\n",
    "        trainer = get_trainer(seq_model, tokenizer, training_args, test_ds, test_ds)\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = trainer.predict(test_ds)\n",
    "        probs = torch.nn.functional.softmax(\n",
    "            torch.tensor(predictions.predictions), dim=1\n",
    "        ).numpy()\n",
    "\n",
    "        all_test_predictions.append(probs)\n",
    "\n",
    "        # Clean up\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "\n",
    "    # Ensemble predictions (simple average)\n",
    "    ensemble_predictions = np.mean(all_test_predictions, axis=0)\n",
    "\n",
    "    # Generate submission\n",
    "    topk = np.argsort(-ensemble_predictions, axis=1)[:, :TOP_K]\n",
    "    flat_topk = topk.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_topk)\n",
    "    topk_labels = decoded_labels.reshape(topk.shape)\n",
    "\n",
    "    joined_preds = [\" \".join(row) for row in topk_labels]\n",
    "\n",
    "    submission = pd.DataFrame(\n",
    "        {\"row_id\": test_df.row_id.values, \"Category:Misconception\": joined_preds}\n",
    "    )\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"Test predictions saved to 'submission.csv'\")\n",
    "    return ensemble_predictions, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ef6168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING TEST PREDICTIONS\n",
      "============================================================\n",
      "Loading fold 1 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28fd2fa09db4c5591ed150e91f330e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600c2d89a319433187a6c1885ac2e866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "test_predictions, submission = generate_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e35de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Category:Misconception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36696</td>\n",
       "      <td>True_Correct:NA True_Neither:NA False_Correct:NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36697</td>\n",
       "      <td>False_Misconception:WNB False_Neither:NA False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36698</td>\n",
       "      <td>True_Neither:NA True_Correct:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                             Category:Misconception\n",
       "0   36696   True_Correct:NA True_Neither:NA False_Correct:NA\n",
       "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
       "2   36698  True_Neither:NA True_Correct:NA True_Misconcep..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529a52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
