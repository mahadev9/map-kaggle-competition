{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a9878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "if \"/kaggle\" in ROOT_PATH:\n",
    "    ROOT_PATH = \"/kaggle/input\"\n",
    "    sys.path.append(os.path.join(ROOT_PATH, \"map-utilities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdd51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    stringify_input,\n",
    "    get_model_name,\n",
    "    get_sequence_classifier,\n",
    "    get_tokenizer,\n",
    "    get_training_arguments,\n",
    "    get_trainer,\n",
    "    convert_latex_to_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL = \"microsoft/deberta-v3-large\"\n",
    "BASE_MODEL = \"answerdotai/ModernBERT-large\"\n",
    "# BASE_MODEL = \"jhu-clsp/ettin-encoder-1b\"\n",
    "# BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "# BASE_MODEL = \"google/gemma-2-9b-it\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-1.7B\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-8B\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-14B\"\n",
    "# BASE_MODEL = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "# BASE_MODEL = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "# BASE_MODEL = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "# BASE_MODEL = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "# BASE_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-Embedding-4B\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-Embedding-8B\"\n",
    "# BASE_MODEL = \"nvidia/AceMath-1.5B-Instruct\"\n",
    "# BASE_MODEL = \"nvidia/AceReason-Nemotron-1.1-7B\"\n",
    "# BASE_MODEL = \"nvidia/AceReason-Nemotron-14B\"\n",
    "# BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# BASE_MODEL = \"google/t5gemma-l-l-ul2-it\"\n",
    "# BASE_MODEL = \"google/t5gemma-2b-2b-ul2-it\"\n",
    "# BASE_MODEL = \"google/t5gemma-9b-2b-ul2-it\"\n",
    "# BASE_MODEL = \"google/gemma-3-1b-it\"\n",
    "# BASE_MODEL = \"google/gemma-3-12b-it\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "TOP_K = 10\n",
    "SPLIT_RATIO = 0.2\n",
    "MAX_LEN = 256\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 16\n",
    "MODEL_NAME = get_model_name(\"/kaggle\" in ROOT_PATH, ROOT_PATH, BASE_MODEL)\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = False\n",
    "BITS = 4\n",
    "USE_4BIT = BITS == 4\n",
    "USE_8BIT = BITS == 8\n",
    "\n",
    "TRAIN_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"train.csv\")\n",
    "TEST_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737e7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ec8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (36696, 7)\n",
      "Testing Shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape:\", train_df.shape)\n",
    "print(\"Testing Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defda15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Misconception = train_df.Misconception.fillna(\"NA\")\n",
    "train_df[\"predict\"] = train_df.Category + \":\" + train_df.Misconception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9836c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1944120/1307861462.py:10: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df.is_mc_answer_correct = train_df.is_mc_answer_correct.fillna(False)\n",
      "/tmp/ipykernel_1944120/1307861462.py:13: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df.is_mc_answer_correct = test_df.is_mc_answer_correct.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "idx = train_df.Category.str.contains(\"True\", case=False)\n",
    "tmp = train_df.loc[idx].copy()\n",
    "tmp[\"c\"] = tmp.groupby([\"QuestionId\", \"MC_Answer\"]).MC_Answer.transform(\"count\")\n",
    "tmp = tmp.sort_values(\"c\", ascending=False)\n",
    "tmp = tmp.drop_duplicates([\"QuestionId\"])\n",
    "tmp = tmp[[\"QuestionId\", \"MC_Answer\"]]\n",
    "tmp[\"is_mc_answer_correct\"] = True\n",
    "\n",
    "train_df = train_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "train_df.is_mc_answer_correct = train_df.is_mc_answer_correct.fillna(False)\n",
    "\n",
    "test_df = test_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "test_df.is_mc_answer_correct = test_df.is_mc_answer_correct.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82e38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"is_student_explanation_correct\"] = train_df.Category.apply(\n",
    "    lambda x: 0 if \"Neither\" in x else (1 if \"Correct\" in x else 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f3a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (36696, 11) with 65 predict classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/map-kaggle-competition/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# le = LabelEncoder()\n",
    "le = joblib.load(os.path.join(ROOT_PATH, \"label_encoder.joblib\"))\n",
    "\n",
    "train_df[\"label\"] = le.transform(train_df[\"predict\"])\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train_df.shape} with {n_classes} predict classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af5b278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>MC_Answer</th>\n",
       "      <th>StudentExplanation</th>\n",
       "      <th>Category</th>\n",
       "      <th>Misconception</th>\n",
       "      <th>predict</th>\n",
       "      <th>is_mc_answer_correct</th>\n",
       "      <th>is_student_explanation_correct</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>0ne third is equal to tree nineth</td>\n",
       "      <td>True_Correct</td>\n",
       "      <td>NA</td>\n",
       "      <td>True_Correct:NA</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>1 / 3 because 6 over 9 is 2 thirds and 1 third...</td>\n",
       "      <td>True_Correct</td>\n",
       "      <td>NA</td>\n",
       "      <td>True_Correct:NA</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>1 3rd is half of 3 6th, so it is simplee to un...</td>\n",
       "      <td>True_Neither</td>\n",
       "      <td>NA</td>\n",
       "      <td>True_Neither:NA</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>1 goes into everything and 3 goes into nine</td>\n",
       "      <td>True_Neither</td>\n",
       "      <td>NA</td>\n",
       "      <td>True_Neither:NA</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>1 out of every 3 isn't coloured</td>\n",
       "      <td>True_Correct</td>\n",
       "      <td>NA</td>\n",
       "      <td>True_Correct:NA</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  QuestionId                                       QuestionText  \\\n",
       "0       0       31772  What fraction of the shape is not shaded? Give...   \n",
       "1       1       31772  What fraction of the shape is not shaded? Give...   \n",
       "2       2       31772  What fraction of the shape is not shaded? Give...   \n",
       "3       3       31772  What fraction of the shape is not shaded? Give...   \n",
       "4       4       31772  What fraction of the shape is not shaded? Give...   \n",
       "\n",
       "           MC_Answer                                 StudentExplanation  \\\n",
       "0  \\( \\frac{1}{3} \\)                  0ne third is equal to tree nineth   \n",
       "1  \\( \\frac{1}{3} \\)  1 / 3 because 6 over 9 is 2 thirds and 1 third...   \n",
       "2  \\( \\frac{1}{3} \\)  1 3rd is half of 3 6th, so it is simplee to un...   \n",
       "3  \\( \\frac{1}{3} \\)        1 goes into everything and 3 goes into nine   \n",
       "4  \\( \\frac{1}{3} \\)                    1 out of every 3 isn't coloured   \n",
       "\n",
       "       Category Misconception          predict  is_mc_answer_correct  \\\n",
       "0  True_Correct            NA  True_Correct:NA                  True   \n",
       "1  True_Correct            NA  True_Correct:NA                  True   \n",
       "2  True_Neither            NA  True_Neither:NA                  True   \n",
       "3  True_Neither            NA  True_Neither:NA                  True   \n",
       "4  True_Correct            NA  True_Correct:NA                  True   \n",
       "\n",
       "   is_student_explanation_correct  label  \n",
       "0                               1     37  \n",
       "1                               1     37  \n",
       "2                               0     64  \n",
       "3                               0     64  \n",
       "4                               1     37  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93ee45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]',\n",
       "       'Calculate ( (1)/(2) / 6 )',\n",
       "       'A box contains ( 120 ) counters. The counters are red or blue. ( (3)/(5) ) of the counters are red.\\nHow many red counters are there?',\n",
       "       '( (A)/(10)=(9)/(15) ) What is the value of ( A ) ?',\n",
       "       '( 2 y=24 ) What is the value of ( y ) ?',\n",
       "       'Calculate ( (2)/(3) x 5 )', 'Which number is the greatest?',\n",
       "       'A bag contains ( 24 ) yellow and green balls. ( (3)/(8) ) of the balls are yellow. How many of the balls are green?',\n",
       "       '( (1)/(3)+(2)/(5)= )',\n",
       "       'Sally has ( (2)/(3) ) of a whole cake in the fridge. Robert eats ( (1)/(3) ) of this piece. What fraction of the whole cake has Robert eaten?\\nChoose the number sentence that would solve the word problem.',\n",
       "       'This is part of a regular polygon. How many sides does it have? [Image: A diagram showing an obtuse angle labelled 144 degrees]',\n",
       "       'What number belongs in the box?\\n(\\n(-8)-(-5)=\\nsquare)',\n",
       "       'Dots have been arranged in these patterns: [Image: Pattern 1 consists of 6 dots, Pattern 2 consists of 10 dots, Pattern 3 consists of 14 dots and Pattern 4 consists of 18 dots] How many dots would there be in Pattern ( 6 ) ?',\n",
       "       'It takes ( 3 ) people a total of ( 192 ) hours to build a wall.\\n\\nHow long would it take if ( 12 ) people built the same wall?',\n",
       "       'The probability of an event occurring is ( 0.9 ).\\n\\nWhich of the following most accurately describes the likelihood of the event occurring?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.QuestionText.apply(convert_latex_to_text).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16dc89e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['( (1)/(3) )', '( (3)/(6) )', '( (3)/(8) )', '( (3)/(9) )',\n",
       "       '( 3 )', '( (1)/(12) )', '( (6)/(2) )', '( 24 )', '( 48 )',\n",
       "       '( 60 )', '( 72 )', '( 4 )', '( 6 )', '( 9 )', '( 12 )', '( 22 )',\n",
       "       '( 3 (1)/(3) )', '( 5 (2)/(3) )', '( (10)/(15) )', '( (2)/(15) )',\n",
       "       '( 6.0001 )', '( 6.079 )', '( 6.2 )', '( 15 )', '( 8 )',\n",
       "       '( (11)/(15) )', '( (11)/(30) )', '( (3)/(15) )',\n",
       "       '( (1)/(3) x (2)/(3) )', '( (1)/(3)+(2)/(3) )',\n",
       "       '( (2)/(3) / (1)/(3) )', '( (2)/(3)-(1)/(3) )',\n",
       "       'Not enough information', '( 10 )', '( 5 )', '( -13 )', '( -3 )',\n",
       "       '( 13 )', '( 20 )', '( 26 )', '( 36 )', '( 192 ) hours',\n",
       "       '( 48 ) hours', '( 64 ) hours', '( 768 ) hours', 'Certain',\n",
       "       'Impossible', 'Likely', 'Unlikely'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.MC_Answer.apply(convert_latex_to_text).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73972293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_config():\n",
    "    \"\"\"Setup model configuration for each fold\"\"\"\n",
    "    # LoRA configuration\n",
    "    lora_config = None\n",
    "    if USE_LORA:\n",
    "        R = 8\n",
    "        lora_config = LoraConfig(\n",
    "            r=R,\n",
    "            lora_alpha=R * 4,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"down_proj\",\n",
    "                \"up_proj\",\n",
    "                \"gate_proj\",\n",
    "            ],\n",
    "            lora_dropout=0.05,\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "    # Quantization configuration\n",
    "    q_lora_config = {\"torch_dtype\": torch.bfloat16}\n",
    "    if USE_QLORA:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        kwargs = {}\n",
    "        if USE_4BIT:\n",
    "            kwargs = {\n",
    "                \"load_in_4bit\": True,\n",
    "                \"bnb_4bit_quant_type\": \"nf4\",\n",
    "                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                \"bnb_4bit_use_double_quant\": True,\n",
    "                \"bnb_4bit_quant_storage\": torch.bfloat16,\n",
    "            }\n",
    "        if USE_8BIT:\n",
    "            kwargs = {\"load_in_8bit\": True}\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(**kwargs)\n",
    "        q_lora_config[\"quantization_config\"] = bnb_config\n",
    "\n",
    "    return lora_config, q_lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d09fdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    for obj in list(globals().keys()):\n",
    "        if isinstance(globals()[obj], torch.nn.Module) or isinstance(globals()[obj], torch.Tensor):\n",
    "            del globals()[obj]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f70b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map3(predictions, labels):\n",
    "    \"\"\"Calculate MAP@3 score\"\"\"\n",
    "    top3 = np.argsort(-predictions, axis=1)[:, :3]\n",
    "    match = top3 == labels[:, None]\n",
    "    weights = np.array([1.0, 0.5, 1 / 3])\n",
    "    scores = np.sum(match * weights, axis=1)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb86a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_fold(fold_idx, train_idx, val_idx):\n",
    "    \"\"\"Train a single fold\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Training Fold {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(f\"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Create fold datasets\n",
    "    fold_train_df = train_df.iloc[train_idx].copy()\n",
    "    fold_val_df = train_df.iloc[val_idx].copy()\n",
    "\n",
    "    # Prepare string inputs\n",
    "    fold_train_df[\"stringified_input\"] = fold_train_df.apply(\n",
    "        lambda row: stringify_input(row, MODEL_NAME), axis=1\n",
    "    )\n",
    "    fold_val_df[\"stringified_input\"] = fold_val_df.apply(\n",
    "        lambda row: stringify_input(row, MODEL_NAME), axis=1\n",
    "    )\n",
    "\n",
    "    # Create HF datasets\n",
    "    train_ds = Dataset.from_pandas(fold_train_df[[\"stringified_input\", \"label\"]])\n",
    "    val_ds = Dataset.from_pandas(fold_val_df[[\"stringified_input\", \"label\"]])\n",
    "\n",
    "    # Setup model\n",
    "    lora_config, q_lora_config = setup_model_config()\n",
    "    seq_model = get_sequence_classifier(MODEL_NAME, n_classes, q_lora_config)\n",
    "    tokenizer = get_tokenizer(MODEL_NAME)\n",
    "\n",
    "    # Handle padding token\n",
    "    if (\n",
    "        \"gemma\" in MODEL_NAME.lower()\n",
    "        or \"qwen\" in MODEL_NAME.lower()\n",
    "        or \"deepseek-math\" in MODEL_NAME.lower()\n",
    "        or \"llama-3.1\" in MODEL_NAME.lower()\n",
    "        or \"acemath\" in MODEL_NAME.lower()\n",
    "    ):\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        seq_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Apply PEFT\n",
    "    if USE_QLORA:\n",
    "        seq_model = prepare_model_for_kbit_training(seq_model)\n",
    "\n",
    "    if USE_LORA:\n",
    "        seq_model = get_peft_model(seq_model, lora_config)\n",
    "\n",
    "    # Tokenize datasets\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"stringified_input\"])\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "    val_ds = val_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    train_ds.set_format(type=\"torch\", columns=columns)\n",
    "    val_ds.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = get_training_arguments(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        epochs=EPOCHS,\n",
    "        train_batch_size=BATCH_SIZE,\n",
    "        eval_batch_size=BATCH_SIZE*2,\n",
    "        bf16_support=\"/kaggle\" not in ROOT_PATH,\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = get_trainer(\n",
    "        seq_model,\n",
    "        tokenizer,\n",
    "        training_args,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Save fold model\n",
    "    fold_model_path = f\"oof_models/{MODEL_NAME.replace('/', '-')}/fold_{fold_idx}\"\n",
    "    complete_dir = os.path.join(ROOT_PATH, fold_model_path)\n",
    "    if os.path.exists(complete_dir):\n",
    "        shutil.rmtree(complete_dir)\n",
    "    os.makedirs(complete_dir, exist_ok=True)\n",
    "    trainer.save_model(fold_model_path)\n",
    "    tokenizer.save_pretrained(fold_model_path)\n",
    "\n",
    "    # Generate OOF predictions\n",
    "    val_predictions = trainer.predict(val_ds)\n",
    "    val_probs = torch.nn.functional.softmax(\n",
    "        torch.tensor(val_predictions.predictions), dim=1\n",
    "    ).numpy()\n",
    "\n",
    "    # Calculate fold score\n",
    "    val_labels = fold_val_df[\"label\"].values\n",
    "    fold_score = calculate_map3(val_probs, val_labels)\n",
    "\n",
    "    print(f\"Fold {fold_idx + 1} MAP@3: {fold_score:.5f}\")\n",
    "\n",
    "    del seq_model, tokenizer, training_args, trainer\n",
    "    del train_ds, val_ds, fold_train_df, fold_val_df, val_predictions, val_labels\n",
    "    clear_memory()\n",
    "    clear_memory()\n",
    "    clear_memory()\n",
    "    clear_memory()\n",
    "\n",
    "    return val_probs, val_idx, fold_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002eeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_oof_training():\n",
    "    \"\"\"Main OOF training loop\"\"\"\n",
    "    # Setup stratified K-fold\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize OOF predictions\n",
    "    oof_predictions = np.zeros((len(train_df), n_classes))\n",
    "    oof_scores = []\n",
    "\n",
    "    # Train each fold\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(train_df, train_df[\"label\"])\n",
    "    ):\n",
    "        val_probs, val_indices, fold_score = train_single_fold(\n",
    "            fold_idx, train_idx, val_idx\n",
    "        )\n",
    "\n",
    "        # Store OOF predictions\n",
    "        oof_predictions[val_indices] = val_probs\n",
    "        oof_scores.append(fold_score)\n",
    "\n",
    "    # Calculate overall OOF score\n",
    "    overall_score = calculate_map3(oof_predictions, train_df[\"label\"].values)\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"OOF TRAINING COMPLETED\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Individual Fold Scores: {[f'{score:.5f}' for score in oof_scores]}\")\n",
    "    print(f\"Mean Fold Score: {np.mean(oof_scores):.5f} ± {np.std(oof_scores):.5f}\")\n",
    "    print(f\"Overall OOF Score: {overall_score:.5f}\")\n",
    "\n",
    "    # Save OOF predictions\n",
    "    oof_df = pd.DataFrame(\n",
    "        oof_predictions, columns=[f\"pred_{i}\" for i in range(n_classes)]\n",
    "    )\n",
    "    oof_df[\"true_label\"] = train_df[\"label\"].values\n",
    "    oof_df[\"predict\"] = train_df[\"predict\"].values\n",
    "    oof_df[\"fold_score\"] = 0\n",
    "\n",
    "    # Add fold information\n",
    "    fold_info = np.zeros(len(train_df))\n",
    "    for fold_idx, (_, val_idx) in enumerate(skf.split(train_df, train_df[\"label\"])):\n",
    "        fold_info[val_idx] = fold_idx\n",
    "    oof_df[\"fold\"] = fold_info\n",
    "\n",
    "    oof_df.to_csv(\"oof_predictions.csv\", index=False)\n",
    "\n",
    "    return oof_predictions, oof_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad2da980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/map-kaggle-competition/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Fold 1/5\n",
      "Train samples: 29356, Val samples: 7340\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078e67381f3c486ab7d1b65e9d4e0e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8735d905ce634a60be0e5a08cf0f533c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04929b244a9d4fd5b8762f94c2ced978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4828511d39408e87d19c2a60d2ca3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31997a2069934e25a7ecbe02a7938f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1594\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1593\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1596\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m   1600\u001b[39m bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[31mValueError\u001b[39m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:103\u001b[39m, in \u001b[36mDebertaV2TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     90\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     **kwargs,\n\u001b[32m    102\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_lower_case = do_lower_case\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m oof_predictions, oof_scores = \u001b[43mrun_oof_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_oof_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train each fold\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[32m     12\u001b[39m     skf.split(train_df, train_df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     val_probs, val_indices, fold_score = \u001b[43mtrain_single_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Store OOF predictions\u001b[39;00m\n\u001b[32m     19\u001b[39m     oof_predictions[val_indices] = val_probs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_single_fold\u001b[39m\u001b[34m(fold_idx, train_idx, val_idx)\u001b[39m\n\u001b[32m     25\u001b[39m lora_config, q_lora_config = setup_model_config()\n\u001b[32m     26\u001b[39m seq_model = get_sequence_classifier(MODEL_NAME, n_classes, q_lora_config)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m tokenizer = \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Handle padding token\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgemma\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME.lower()\n\u001b[32m     32\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mqwen\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME.lower()\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33macemath\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME.lower()\n\u001b[32m     36\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/src/utils.py:122\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodernbert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name.lower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mettin\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name.lower():\n\u001b[32m    119\u001b[39m     extra_kwargs = {\n\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreference_compile\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    121\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2317\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2316\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/map-kaggle-competition/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "oof_predictions, oof_scores = run_oof_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_predictions():\n",
    "    \"\"\"Generate test predictions using all fold models\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"GENERATING TEST PREDICTIONS\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Prepare test data\n",
    "    test_df[\"stringified_input\"] = test_df.apply(\n",
    "        lambda row: stringify_input(row, MODEL_NAME), axis=1\n",
    "    )\n",
    "\n",
    "    all_test_predictions = []\n",
    "\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        print(f\"Loading fold {fold_idx + 1} model...\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        model_path = model_path = os.path.join(\n",
    "            ROOT_PATH,\n",
    "            \"oof_models\",\n",
    "            MODEL_NAME.replace(\"/\", \"-\"),\n",
    "            f\"fold_{fold_idx}\",\n",
    "        )\n",
    "        if USE_LORA:\n",
    "            model_path = MODEL_NAME\n",
    "        tokenizer = get_tokenizer(model_path)\n",
    "\n",
    "        # Prepare test dataset\n",
    "        test_ds = Dataset.from_pandas(test_df[[\"stringified_input\"]])\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"stringified_input\"])\n",
    "\n",
    "        test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "        # Load model and generate predictions\n",
    "        lora_config, q_lora_config = setup_model_config()\n",
    "        seq_model = get_sequence_classifier(model_path, n_classes, q_lora_config)\n",
    "\n",
    "        # Handle padding token\n",
    "        if (\n",
    "            \"gemma\" in MODEL_NAME.lower()\n",
    "            or \"qwen\" in MODEL_NAME.lower()\n",
    "            or \"deepseek-math\" in MODEL_NAME.lower()\n",
    "            or \"llama-3.1\" in MODEL_NAME.lower()\n",
    "            or \"acemath\" in MODEL_NAME.lower()\n",
    "        ):\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            seq_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        if USE_LORA:\n",
    "            fold_model_path = os.path.join(\n",
    "                ROOT_PATH,\n",
    "                \"oof_models\",\n",
    "                MODEL_NAME.replace(\"/\", \"-\"),\n",
    "                f\"fold_{fold_idx}\",\n",
    "            )\n",
    "            seq_model = PeftModel.from_pretrained(seq_model, fold_model_path)\n",
    "\n",
    "        # Create trainer for inference\n",
    "        training_args = get_training_arguments(\n",
    "            bf16_support=\"/kaggle\" not in ROOT_PATH,\n",
    "            train_on_full_dataset=True,  # No validation needed for inference\n",
    "        )\n",
    "        trainer = get_trainer(seq_model, tokenizer, training_args, test_ds, test_ds)\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = trainer.predict(test_ds)\n",
    "        probs = torch.nn.functional.softmax(\n",
    "            torch.tensor(predictions.predictions), dim=1\n",
    "        ).numpy()\n",
    "\n",
    "        all_test_predictions.append(probs)\n",
    "\n",
    "        # Clean up\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "        clear_memory()\n",
    "\n",
    "    # Ensemble predictions (simple average)\n",
    "    ensemble_predictions = np.mean(all_test_predictions, axis=0)\n",
    "\n",
    "    # Generate submission\n",
    "    topk = np.argsort(-ensemble_predictions, axis=1)[:, :TOP_K]\n",
    "    flat_topk = topk.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_topk)\n",
    "    topk_labels = decoded_labels.reshape(topk.shape)\n",
    "\n",
    "    joined_preds = [\" \".join(row) for row in topk_labels]\n",
    "\n",
    "    submission = pd.DataFrame(\n",
    "        {\"row_id\": test_df.row_id.values, \"Category:Misconception\": joined_preds}\n",
    "    )\n",
    "    submission.to_csv(\"oof_submission.csv\", index=False)\n",
    "\n",
    "    print(\"Test predictions saved to 'oof_submission.csv'\")\n",
    "    return ensemble_predictions, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING TEST PREDICTIONS\n",
      "============================================================\n",
      "Loading fold 1 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40cf12d88e44d7baf3d30d1592e7c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa78960531ef40968e6c6a95d3ca80bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4439e8b7d04f9f9ec0c4ccb98867f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320fcac2048343e485d7729ec7f2256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 3 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf82cf60b06044e581eef95c9468e871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5d691685c64e348d74a08e6884d7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 4 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85322cf8bf24fdbbe9952cdd9ad343c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8151760b584e34841695176c39e925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 5 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832967c89db648299360502f60bf5fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04c25607d7544a1b07a1b001fcadd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/deepseek-math-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/map-kaggle-competition/src/utils.py:226: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'oof_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "test_predictions, submission = generate_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2fb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Category:Misconception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36696</td>\n",
       "      <td>True_Correct:NA True_Neither:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36697</td>\n",
       "      <td>False_Misconception:WNB False_Neither:NA False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36698</td>\n",
       "      <td>True_Neither:NA True_Correct:NA True_Misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id                             Category:Misconception\n",
       "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
       "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
       "2   36698  True_Neither:NA True_Correct:NA True_Misconcep..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd45b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
