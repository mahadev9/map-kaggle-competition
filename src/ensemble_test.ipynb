{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270605c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps /kaggle/input/map-utilities/bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6249ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "if \"/kaggle\" in ROOT_PATH:\n",
    "    ROOT_PATH = \"/kaggle/input\"\n",
    "    sys.path.append(os.path.join(ROOT_PATH, \"map-utilities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from utils import (\n",
    "    stringify_input,\n",
    "    get_model_name,\n",
    "    get_sequence_classifier,\n",
    "    get_tokenizer,\n",
    "    get_training_arguments,\n",
    "    get_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ace201",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = joblib.load(os.path.join(ROOT_PATH, \"map-utilities\", \"label_encoder.joblib\"))\n",
    "n_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "MAX_LEN = 256\n",
    "\n",
    "MODEL_VARIATIONS = [\n",
    "    {\n",
    "        \"model_name\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"qwen-3/transformers/14b/1\"\n",
    "        ),\n",
    "        \"adapter_path\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"qwen3-14b-qlora-4bit/transformers/default/1\"\n",
    "        ),\n",
    "        \"submission_file\": \"submission_qwen3_14b.csv\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"gemma-2/transformers/gemma-2-9b-it/2\"\n",
    "        ),\n",
    "        \"adapter_path\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"gemma2-9b-it-qlora-4bit/transformers/default/1\"\n",
    "        ),\n",
    "        \"submission_file\": \"submission_gemma2_9b.csv\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"deepseek-r1-0528/transformers/deepseek-r1-0528-qwen3-8b/1\"\n",
    "        ),\n",
    "        \"adapter_path\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"deepseek-r1-0528-qwen3-8b-qlora-4bit/transformers/default/1\"\n",
    "        ),\n",
    "        \"submission_file\": \"submission_deepseek_r1_0528_qwen3_8b.csv\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"deepseek-math/pytorch/deepseek-math-7b-instruct/1\"\n",
    "        ),\n",
    "        \"adapter_path\": get_model_name(\n",
    "            \"/kaggle\" in ROOT_PATH, ROOT_PATH, \"deepseek-math-7b-instruct-qlora-4bit/transformers/default/1\"\n",
    "        ),\n",
    "        \"submission_file\": \"submission_deepseek_math_7b.csv\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"train.csv\")\n",
    "TEST_PATH = os.path.join(ROOT_PATH, \"map-charting-student-math-misunderstandings\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Shape:\", train_df.shape)\n",
    "print(\"Testing Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05661b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train_df.Category.str.contains(\"True\", case=False)\n",
    "tmp = train_df.loc[idx].copy()\n",
    "tmp[\"c\"] = tmp.groupby([\"QuestionId\", \"MC_Answer\"]).MC_Answer.transform(\"count\")\n",
    "tmp = tmp.sort_values(\"c\", ascending=False)\n",
    "tmp = tmp.drop_duplicates([\"QuestionId\"])\n",
    "tmp = tmp[[\"QuestionId\", \"MC_Answer\"]]\n",
    "tmp[\"is_mc_answer_correct\"] = True\n",
    "\n",
    "train_df = train_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "train_df.is_mc_answer_correct = train_df.is_mc_answer_correct.fillna(False)\n",
    "\n",
    "test_df = test_df.merge(tmp, on=[\"QuestionId\", \"MC_Answer\"], how=\"left\")\n",
    "test_df.is_mc_answer_correct = test_df.is_mc_answer_correct.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_config = {\n",
    "    \"torch_dtype\": torch.float16,\n",
    "    \"device_map\": \"auto\"\n",
    "}\n",
    "if USE_QLORA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.float16,\n",
    "    )\n",
    "    qlora_config[\"quantization_config\"] = bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(model_name, adapter_path, submission_file):\n",
    "    seq_model = get_sequence_classifier(model_name, n_classes, qlora_config)\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"stringified_input\"], truncation=True)\n",
    "\n",
    "    if (\n",
    "        \"gemma\" in model_name.lower()\n",
    "        or \"qwen\" in model_name.lower()\n",
    "        or \"deepseek-math\" in model_name.lower()\n",
    "    ):\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        seq_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    if USE_LORA:\n",
    "        seq_model = PeftModel.from_pretrained(seq_model, adapter_path)\n",
    "\n",
    "    test_df[\"stringified_input\"] = test_df.apply(\n",
    "        lambda row: stringify_input(row, model_name), axis=1\n",
    "    )\n",
    "\n",
    "    test_ds = Dataset.from_pandas(test_df[[\"stringified_input\"]])\n",
    "    test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = get_training_arguments(bf16_support=\"/kaggle\" not in ROOT_PATH)\n",
    "    trainer = get_trainer(\n",
    "        seq_model,\n",
    "        tokenizer,\n",
    "        training_args,\n",
    "        test_ds,\n",
    "        test_ds,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(test_ds)\n",
    "    probs = torch.nn.functional.softmax(\n",
    "        torch.tensor(predictions.predictions), dim=1\n",
    "    ).numpy()\n",
    "\n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]\n",
    "\n",
    "    flat_top3 = top3.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_top3)\n",
    "    top3_labels = decoded_labels.reshape(top3.shape)\n",
    "\n",
    "    joined_preds = [\"|\".join(row) for row in top3_labels]\n",
    "\n",
    "    sub = pd.DataFrame(\n",
    "        {\"row_id\": test_df.row_id.values, \"Category:Misconception\": joined_preds}\n",
    "    )\n",
    "    sub.to_csv(submission_file, index=False)\n",
    "\n",
    "    del seq_model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_variation in MODEL_VARIATIONS:\n",
    "    predict_test_data(\n",
    "        model_name=model_variation[\"model_name\"],\n",
    "        adapter_path=model_variation[\"adapter_path\"],\n",
    "        submission_file=model_variation[\"submission_file\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cb2f8",
   "metadata": {},
   "source": [
    "### Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/code/bibanh/lb-0-944-the-art-of-ensemble#4.-ENSEMBLE-EVERYTHING\n",
    "def get_top_k_ensemble(list_of_predictions, k=3):\n",
    "    predictions = []\n",
    "    weights = []\n",
    "    for lp in list_of_predictions:\n",
    "        predictions.append(lp.split(\"|\"))\n",
    "        weights.append(4)\n",
    "    score = defaultdict(int)\n",
    "\n",
    "    for i, lst in enumerate(predictions):\n",
    "        weight = weights[i]\n",
    "        for rank, item in enumerate(lst):\n",
    "            score[item] += (len(lst) - rank) * weight\n",
    "\n",
    "    sorted_items = sorted(score.items(), key=lambda x: -x[1])\n",
    "    return ' '.join([item for item, _ in sorted_items[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfa628",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_variation in MODEL_VARIATIONS:\n",
    "    df = pd.read_csv(model_variation[\"submission_file\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "ensemble_df = dfs[0][['row_id']].copy()\n",
    "for i, df in enumerate(dfs):\n",
    "    model_name = MODEL_VARIATIONS[i][\"submission_file\"].replace(\"submission_\", \"\").replace(\".csv\", \"\")\n",
    "    ensemble_df[f\"predictions_{model_name}\"] = df[\"Category:Misconception\"]\n",
    "\n",
    "print(\"Ensemble df shape:\", ensemble_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df[\"Category:Misconception\"] = ensemble_df.apply(\n",
    "    lambda row: get_top_k_ensemble(\n",
    "        [row[f\"predictions_{model_name}\"] for model_name in MODEL_VARIATIONS],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "ensemble_df[['row_id', 'Category:Misconception']].to_csv('submission.csv', index = False)\n",
    "pd.read_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
